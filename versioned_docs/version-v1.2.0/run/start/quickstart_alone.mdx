---
sidebar_position: 2
description: Create a DV alone
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Create a DV Alone

:::info
It is possible for a single operator to manage all of the nodes of a DV cluster. The nodes can be run on a single machine, which is only suitable for testing, or the nodes can be run on multiple machines, which is expected for a production setup.

The private key shares can be created centrally and distributed securely to each node. Alternatively, the private key shares can be created in a lower-trust manner with a [Distributed Key Generation](../../learn/intro/key-concepts.md#distributed-validator-key-generation-ceremony) process, which avoids the validator private key being stored in full anywhere, at any point in its lifecycle. Follow the [group quickstart](./quickstart_group.mdx) instead for this latter case.
:::

## Pre-requisites
  <ul>
    <li>A basic <a href="https://docs.ethstaker.cc/ethstaker-knowledge-base/" target="_blank">knowledge</a> of Ethereum nodes and validators.</li>
    <li>Ensure you have <a href="https://git-scm.com/downloads" target="_blank">git</a> installed.</li>
    <li>Ensure you have <a href="https://docs.docker.com/engine/install/" target="_blank">docker</a> installed.</li>
    <li>Make sure <code>docker</code> is running before executing the commands below.</li>
  </ul>

## Step 1: Create the key shares locally

<Tabs groupId="Launchpad-other">
  <TabItem value="Launchpad" label="Launchpad" default>
    Go to the the <a href="/docs/dvl/intro#dv-launchpad-links">DV Launchpad</a> and select <code>Create a distributed validator alone</code>. Follow the steps to configure your DV cluster. The Launchpad will give you a docker command to create your cluster. <br/>Before you run the command, clone the <a href="https://github.com/ObolNetwork/charon-distributed-validator-cluster.git">CDVC repo</a> and <code>cd</code> into the directory.

  ```shell
  # Clone the repo
  git clone https://github.com/ObolNetwork/charon-distributed-validator-cluster.git

  # Change directory
  cd charon-distributed-validator-cluster/

  # Run the command provided in the DV Launchpad "Create a cluster alone" flow
  docker run -u $(id -u):$(id -g) --rm -v "$(pwd)/:/opt/charon" obolnetwork/charon:v1.2.0 create cluster --definition-file=...
  ```

  </TabItem>

  <TabItem value="CLI" label="CLI">

1. Clone the <a href="https://github.com/ObolNetwork/charon-distributed-validator-cluster">CDVC repo</a> and <code>cd</code> into the directory.

  ```shell
  # Clone the repo
  git clone https://github.com/ObolNetwork/charon-distributed-validator-cluster.git

  # Change directory
  cd charon-distributed-validator-cluster/
  ```

2. Run the cluster creation command, setting required flag values.

  Run the below command to create the validator private key shares and cluster artifacts locally, replacing the example values for `nodes`, `network`, `num-validators`, `fee-recipient-addresses`,  and `withdrawal-addresses`.
  Check the [Charon CLI reference](../../learn/charon/charon-cli-reference.md#create-a-full-cluster-locally) for additional, optional flags to set.

  ```shell
    docker run --rm -v "$(pwd):/opt/charon" obolnetwork/charon:v1.2.0 create cluster \
      --nodes=4 \
      --network=holesky \
      --num-validators=1 \
      --name="Quickstart Guide Cluster" \
      --cluster-dir="cluster" \
      --fee-recipient-addresses=0x000000000000000000000000000000000000dead \
      --withdrawal-addresses=0x000000000000000000000000000000000000dead
  ```

:::tip
If you would like your cluster to appear on the [DV Launchpad](../../learn/intro/launchpad.md), add the `--publish` flag to the command.
:::

  </TabItem>
</Tabs>
<br />

After the `create cluster` command is run, you should have multiple subfolders within the newly created `./cluster/` folder, one for each node created.

**Backup the `./cluster/` folder, then move on to deploying the cluster.**

:::info
Make sure your backup is secure and private, someone with access to these files could get the validators slashed.
:::

## Step 2: Deploy and start the nodes

<Tabs groupId="Local nodes-distributed nodes">
  <TabItem value="Run the nodes on a single machine" label="Run the nodes on a single machine" default>

:::warning
This part of the guide only runs one Execution Client, one Consensus Client, and 6 Distributed Validator Charon Client + Validator Client pairs on a single docker instance, and **is not suitable for a mainnet deployment**. (If this machine fails, there will not be any fault tolerance - the cluster will also fail.)

For a production deployment with fault tolerance, follow the part of the guide instructing you how to distribute the nodes across multiple machines.
:::

Run this command to start your cluster containers if you deployed using the [CDVC repo](https://github.com/ObolNetwork/charon-distributed-validator-cluster).

```shell
# Start the distributed validator cluster
docker compose up --build -d
```

Check the monitoring dashboard and see if things look all right.

```shell
# Open Grafana
open http://localhost:3000/d/laEp8vupp
```

  </TabItem>
  <TabItem value="Run the nodes on many machines" label="Run the nodes on multiple machines">

:::warning
To distribute your cluster across multiple machines, each node in the cluster needs one of the folders called `node*/` to be copied to it. Each folder should be copied to a [CDVN repo](https://github.com/ObolNetwork/charon-distributed-validator-node) and renamed from `node*` to `.charon`.

Right now, the `charon create cluster` command [used earlier to create the private keys](./quickstart_alone#step-1-create-the-key-shares-locally) outputs a folder structure like `cluster/node*/`. Make sure to grab the `./node*/` folders, *rename* them to `.charon` and then move them to one of the single node repos below. Once all nodes are online, synced, and connected, you will be ready to activate your validator.
:::

 This is necessary for the folder to be found by the default `charon run` command. Optionally, it is possible to override `charon run`'s default file locations by using `charon run --private-key-file="node0/charon-enr-private-key" --lock-file="node0/cluster-lock.json"` for each instance of Charon you start (substituting `node0` for each node number in your cluster as needed).

 :point_right: Use the single node [docker compose](https://github.com/ObolNetwork/charon-distributed-validator-node), the kubernetes [manifests](https://github.com/ObolNetwork/charon-k8s-distributed-validator-node), or the [helm chart](https://github.com/ObolNetwork/helm-charts) example repos to get your nodes up and connected after loading the `.charon` folder artifacts into them appropriately.
<br />

```log title="Output from create cluster"
cluster
├── node0
│   ├── charon-enr-private-key
│   ├── cluster-lock.json
│   ├── deposit-data.json
│   └── validator_keys
│       ├── keystore-0.json
│       ├── keystore-0.txt
│       ├── ...
│       ├── keystore-N.json
│       └── keystore-N.txt
├── node1
│   ├── charon-enr-private-key
│   ├── cluster-lock.json
│   ├── deposit-data.json
│   └── validator_keys
│       ├── keystore-0.json
│       ├── keystore-0.txt
│       ├── ...
│       ├── keystore-N.json
│       └── keystore-N.txt
├── node2
│   ├── charon-enr-private-key
│   ├── cluster-lock.json
│   ├── deposit-data.json
│   └── validator_keys
│       ├── keystore-0.json
│       ├── keystore-0.txt
│       ├── ...
│       ├── keystore-N.json
│       └── keystore-N.txt
└── node3
    ├── charon-enr-private-key
    ├── cluster-lock.json
    ├── deposit-data.json
    └── validator_keys
        ├── keystore-0.json
        ├── keystore-0.txt
        ├── ...
        ├── keystore-N.json
        └── keystore-N.txt

```

```log title="Folder structure to be placed on each DV node"
└── .charon
    ├── charon-enr-private-key
    ├── cluster-lock.json
    ├── deposit-data.json
    └── validator_keys
        ├── keystore-0.json
        ├── keystore-0.txt
        ├── ...
        ├── keystore-N.json
        └── keystore-N.txt
```

:::info
  Currently, the quickstart repo installs a node on the Holesky testnet. It is possible to choose a different network (another testnet, or mainnet) by overriding the `.env` file.

  `.env.sample` is a sample environment file that allows overriding default configuration defined in `docker-compose.yml`. Uncomment and set any variable to override its value.

  Setup the desired inputs for the DV, including the network you wish to operate on. Check the [Charon CLI reference](../../learn/charon/charon-cli-reference.md) for additional optional flags to set. Once you have set the values you wish to use. Make a copy of this file called `.env`.

  ```shell
  # Copy ".env.sample", renaming it ".env"
  cp .env.sample.holesky .env
  ```

:::

  </TabItem>
</Tabs>
